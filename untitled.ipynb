{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "70198547",
   "metadata": {},
   "source": [
    "##  Creating an NLP Data Loader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a38e7ce",
   "metadata": {},
   "source": [
    "As an AI engineer working on a cutting-edge language translation project, we are tasked with bridging the communication gap between speakers of different languages. Translating languages is no small feat, especially given the intricacies, nuances, and cultural contexts embedded within them. Central to the success of this endeavor is the data-large corpora of bilingual sentences that serve as the bedrock of our models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e66f4ca0",
   "metadata": {},
   "source": [
    "In PyTorch, the data loader plays an indispensable role in managing this vast amount of data. For natural language processing (NLP) tasks like ours, data often comes in variable lengths due to differing sentence structures and lengths across languages. The data loader efficiently batches these variable-length sequences, ensuring that our models are trained on diverse examples in every iteration. This batching is crucial for harnessing the power of parallel computation on GPUs, thus expediting the training process."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "248c710b",
   "metadata": {},
   "source": [
    "Furthermore, the data loader aids in shuffling the dataset, which is vital for preventing models from memorizing the sequence of training data and promoting better generalization. Especially for NLP tasks, where data might be ordered or clustered by topics, shuffling ensures that the model remains robust and doesn't develop baises based on the order of input."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e500633b",
   "metadata": {},
   "source": [
    "Lastly, in the world of NLP, preprocessing steps such as tokenization, padding, and numericalization are paramount. The data loader in PyTorch provides hooks that allow us to seamlessly integrate these preprocessing steps, ensuring that the raw textual data is transformed into a format that is amenable for deep learning models. In PyTorch, the data loade plays an indispensable role in managing this vast amount of data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbaeea2c",
   "metadata": {},
   "source": [
    "### Installing required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2b865cfd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  error: subprocess-exited-with-error\n",
      "  \n",
      "  × pip subprocess to install build dependencies did not run successfully.\n",
      "  │ exit code: 1\n",
      "  ╰─> [113 lines of output]\n",
      "      Ignoring numpy: markers 'python_version >= \"3.9\"' don't match your environment\n",
      "      Collecting setuptools\n",
      "        Using cached setuptools-75.3.2-py3-none-any.whl.metadata (6.9 kB)\n",
      "      Collecting cython<3.0,>=0.25\n",
      "        Using cached Cython-0.29.37-py2.py3-none-any.whl.metadata (3.1 kB)\n",
      "      Collecting cymem<2.1.0,>=2.0.2\n",
      "        Using cached cymem-2.0.11.tar.gz (10 kB)\n",
      "        Installing build dependencies: started\n",
      "        Installing build dependencies: finished with status 'done'\n",
      "        Getting requirements to build wheel: started\n",
      "        Getting requirements to build wheel: finished with status 'done'\n",
      "        Preparing metadata (pyproject.toml): started\n",
      "        Preparing metadata (pyproject.toml): finished with status 'done'\n",
      "      Collecting preshed<3.1.0,>=3.0.2\n",
      "        Using cached preshed-3.0.11.tar.gz (15 kB)\n",
      "        Installing build dependencies: started\n",
      "        Installing build dependencies: finished with status 'error'\n",
      "        error: subprocess-exited-with-error\n",
      "      \n",
      "        × pip subprocess to install build dependencies did not run successfully.\n",
      "        │ exit code: 1\n",
      "        ╰─> [81 lines of output]\n",
      "            Collecting setuptools\n",
      "              Using cached setuptools-75.3.2-py3-none-any.whl.metadata (6.9 kB)\n",
      "            Collecting cython>=0.28\n",
      "              Using cached cython-3.2.1-cp38-cp38-win_amd64.whl.metadata (6.6 kB)\n",
      "            Collecting cymem<2.1.0,>=2.0.2\n",
      "              Using cached cymem-2.0.11.tar.gz (10 kB)\n",
      "              Installing build dependencies: started\n",
      "              Installing build dependencies: finished with status 'done'\n",
      "              Getting requirements to build wheel: started\n",
      "              Getting requirements to build wheel: finished with status 'done'\n",
      "              Preparing metadata (pyproject.toml): started\n",
      "              Preparing metadata (pyproject.toml): finished with status 'done'\n",
      "            Collecting murmurhash<1.1.0,>=0.28.0\n",
      "              Using cached murmurhash-1.0.15.tar.gz (13 kB)\n",
      "              Installing build dependencies: started\n",
      "              Installing build dependencies: finished with status 'done'\n",
      "              Getting requirements to build wheel: started\n",
      "              Getting requirements to build wheel: finished with status 'done'\n",
      "              Preparing metadata (pyproject.toml): started\n",
      "              Preparing metadata (pyproject.toml): finished with status 'done'\n",
      "            Using cached setuptools-75.3.2-py3-none-any.whl (1.3 MB)\n",
      "            Using cached cython-3.2.1-cp38-cp38-win_amd64.whl (2.8 MB)\n",
      "            Building wheels for collected packages: cymem, murmurhash\n",
      "              Building wheel for cymem (pyproject.toml): started\n",
      "              Building wheel for cymem (pyproject.toml): finished with status 'error'\n",
      "              error: subprocess-exited-with-error\n",
      "      \n",
      "              × Building wheel for cymem (pyproject.toml) did not run successfully.\n",
      "              │ exit code: 1\n",
      "              ╰─> [15 lines of output]\n",
      "                  running bdist_wheel\n",
      "                  running build\n",
      "                  running build_py\n",
      "                  creating build\\lib.win-amd64-cpython-38\\cymem\n",
      "                  copying cymem\\about.py -> build\\lib.win-amd64-cpython-38\\cymem\n",
      "                  copying cymem\\__init__.py -> build\\lib.win-amd64-cpython-38\\cymem\n",
      "                  creating build\\lib.win-amd64-cpython-38\\cymem\\tests\n",
      "                  copying cymem\\tests\\test_import.py -> build\\lib.win-amd64-cpython-38\\cymem\\tests\n",
      "                  copying cymem\\tests\\__init__.py -> build\\lib.win-amd64-cpython-38\\cymem\\tests\n",
      "                  copying cymem\\cymem.pyx -> build\\lib.win-amd64-cpython-38\\cymem\n",
      "                  copying cymem\\cymem.pxd -> build\\lib.win-amd64-cpython-38\\cymem\n",
      "                  copying cymem\\__init__.pxd -> build\\lib.win-amd64-cpython-38\\cymem\n",
      "                  running build_ext\n",
      "                  building 'cymem.cymem' extension\n",
      "                  error: Microsoft Visual C++ 14.0 or greater is required. Get it with \"Microsoft C++ Build Tools\": https://visualstudio.microsoft.com/visual-cpp-build-tools/\n",
      "                  [end of output]\n",
      "      \n",
      "              note: This error originates from a subprocess, and is likely not a problem with pip.\n",
      "              ERROR: Failed building wheel for cymem\n",
      "              Building wheel for murmurhash (pyproject.toml): started\n",
      "              Building wheel for murmurhash (pyproject.toml): finished with status 'error'\n",
      "              error: subprocess-exited-with-error\n",
      "      \n",
      "              × Building wheel for murmurhash (pyproject.toml) did not run successfully.\n",
      "              │ exit code: 1\n",
      "              ╰─> [19 lines of output]\n",
      "                  running bdist_wheel\n",
      "                  running build\n",
      "                  running build_py\n",
      "                  creating build\\lib.win-amd64-cpython-38\\murmurhash\n",
      "                  copying murmurhash\\about.py -> build\\lib.win-amd64-cpython-38\\murmurhash\n",
      "                  copying murmurhash\\__init__.py -> build\\lib.win-amd64-cpython-38\\murmurhash\n",
      "                  creating build\\lib.win-amd64-cpython-38\\murmurhash\\tests\n",
      "                  copying murmurhash\\tests\\test_hash.py -> build\\lib.win-amd64-cpython-38\\murmurhash\\tests\n",
      "                  copying murmurhash\\tests\\test_import.py -> build\\lib.win-amd64-cpython-38\\murmurhash\\tests\n",
      "                  copying murmurhash\\tests\\__init__.py -> build\\lib.win-amd64-cpython-38\\murmurhash\\tests\n",
      "                  copying murmurhash\\mrmr.pyx -> build\\lib.win-amd64-cpython-38\\murmurhash\n",
      "                  copying murmurhash\\mrmr.pxd -> build\\lib.win-amd64-cpython-38\\murmurhash\n",
      "                  copying murmurhash\\__init__.pxd -> build\\lib.win-amd64-cpython-38\\murmurhash\n",
      "                  creating build\\lib.win-amd64-cpython-38\\murmurhash\\include\\murmurhash\n",
      "                  copying murmurhash\\include\\murmurhash\\MurmurHash2.h -> build\\lib.win-amd64-cpython-38\\murmurhash\\include\\murmurhash\n",
      "                  copying murmurhash\\include\\murmurhash\\MurmurHash3.h -> build\\lib.win-amd64-cpython-38\\murmurhash\\include\\murmurhash\n",
      "                  running build_ext\n",
      "                  building 'murmurhash.mrmr' extension\n",
      "                  error: Microsoft Visual C++ 14.0 or greater is required. Get it with \"Microsoft C++ Build Tools\": https://visualstudio.microsoft.com/visual-cpp-build-tools/\n",
      "                  [end of output]\n",
      "      \n",
      "              note: This error originates from a subprocess, and is likely not a problem with pip.\n",
      "              ERROR: Failed building wheel for murmurhash\n",
      "            Failed to build cymem murmurhash\n",
      "            ERROR: ERROR: Failed to build installable wheels for some pyproject.toml based projects (cymem, murmurhash)\n",
      "            [end of output]\n",
      "      \n",
      "        note: This error originates from a subprocess, and is likely not a problem with pip.\n",
      "      error: subprocess-exited-with-error\n",
      "      \n",
      "      × pip subprocess to install build dependencies did not run successfully.\n",
      "      │ exit code: 1\n",
      "      ╰─> See above for output.\n",
      "      \n",
      "      note: This error originates from a subprocess, and is likely not a problem with pip.\n",
      "      [end of output]\n",
      "  \n",
      "  note: This error originates from a subprocess, and is likely not a problem with pip.\n",
      "error: subprocess-exited-with-error\n",
      "\n",
      "× pip subprocess to install build dependencies did not run successfully.\n",
      "│ exit code: 1\n",
      "╰─> See above for output.\n",
      "\n",
      "note: This error originates from a subprocess, and is likely not a problem with pip.\n",
      "c:\\Users\\sekai\\.conda\\envs\\myenv\\python.exe: No module named spacy\n",
      "c:\\Users\\sekai\\.conda\\envs\\myenv\\python.exe: No module named spacy\n",
      "c:\\Users\\sekai\\.conda\\envs\\myenv\\python.exe: No module named spacy\n"
     ]
    }
   ],
   "source": [
    "!pip install -Uqq torchtext==0.14.1\n",
    "!pip install -Uqq torch==1.13.1\n",
    "!pip install -Uqq spacy\n",
    "!pip install -Uqq torchdata==0.5.1\n",
    "!pip install -Uqq portalocker>=2.0.0\n",
    "!python -m spacy download en_core_web_sm -qq\n",
    "!python -m spacy download de_core_news_sm -qq\n",
    "!python -m spacy download fr_core_news_sm -qq"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15d7c152",
   "metadata": {},
   "source": [
    "### Importing required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0c291296",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\sekai\\.conda\\envs\\myenv\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "from torchtext.datasets import multi30k, Multi30k\n",
    "from typing import Iterable, List\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.utils.data import DataLoader\n",
    "from torchdata.datapipes.iter import IterableWrapper, Mapper\n",
    "import torchtext\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "import numpy as np\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30e44189",
   "metadata": {},
   "source": [
    "### **Custom data set and data loader in PyTorch**\n",
    "In this code snippet, we will see how to create a custom data set and use the DataLoader class in PyTorch. The data set consists of a list of random sentences, and the objective is to create batches of sentences for further processing, such as training a neural network model.\n",
    "\n",
    "We will begin by defining a custom data set called CustomDataset. This data set inherits from the `torch.utils.data.Dataset` class and is initialized with a list of sentences. The data set comprises two essential methods:\n",
    "\n",
    "- __init__(self, sentences): Initializes the data set with a list of sentences.\n",
    "- __getitem__(self, idx): Retrieves an item (in this case, a sentence) at a specific index, idx.\n",
    "\n",
    "Next, we will create an instance of our custom data set (custom_dataset) by passing in the list of sentences. Additionally, we can specify a batch size (batch_size), which determines how many sentences will be grouped together in each batch during data loading.\n",
    "\n",
    "We will then create a DataLoader (dataloader) by providing our custom data set and batch size to the torch.utils.data.DataLoader class. Furthermore, we will set shuffle=True, indicating that the sentences will be randomly shuffled before being divided into batches. This shuffling is particularly useful for training deep learning models, as it prevents the model from learning patterns based on the order of the data.\n",
    "\n",
    "Finally, we will iterate through the DataLoader to demonstrate how data is loaded in batches. In this code, we will see that batch size is set to 2, meaning that each batch will contain two sentences. The DataLoader efficiently manages the loading of data in batches, making it suitable for training deep learning models.\n",
    "\n",
    "During iteration, the sentences in each batch are printed to illustrate how the DataLoader groups and presents the data. This code snippet provides a fundamental example of how to set up a custom data set and data loader in PyTorch, which is a common practice in deep learning workflows.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8028e3e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['It is our choices, Harry, that show what we truly are, far more than our abilities.', \"Fame's a fickle friend, Harry.\"]\n",
      "['Youth can not know how age thinks and feels. But old men are guilty if they forget what it was to be young.', 'You are awesome!']\n",
      "['Soon we must all face the choice between what is right and what is easy.', \"If you want to know what a man's like, take a good look at how he treats his inferiors, not his equals.\"]\n"
     ]
    }
   ],
   "source": [
    "sentences = [\n",
    "    \"If you want to know what a man's like, take a good look at how he treats his inferiors, not his equals.\",\n",
    "    \"Fame's a fickle friend, Harry.\",\n",
    "    \"It is our choices, Harry, that show what we truly are, far more than our abilities.\",\n",
    "    \"Soon we must all face the choice between what is right and what is easy.\",\n",
    "    \"Youth can not know how age thinks and feels. But old men are guilty if they forget what it was to be young.\",\n",
    "    \"You are awesome!\"\n",
    "]\n",
    "\n",
    "# Define a custom dataset\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, sentences):\n",
    "        self.sentences = sentences\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.sentences)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.sentences[idx]\n",
    "    \n",
    "# Create an instance of our custom dataset \n",
    "custom_dataset = CustomDataset(sentences)\n",
    "\n",
    "# Define batch size \n",
    "batch_size = 2\n",
    "\n",
    "# Create a DataLoader \n",
    "dataloader = DataLoader(custom_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# Iterate through the DataLoader \n",
    "for batch in dataloader:\n",
    "    print(batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "575f926f",
   "metadata": {},
   "source": [
    "As shown above, the data is organized into batches of 2 sentences each. The next step is to convert these sentences into tensors. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "febf5c47",
   "metadata": {},
   "source": [
    "### Creating tensors for custom data set\n",
    "\n",
    "In this code example, we will see the creation of a custom data set for natural language processing (NLP) tasks using PyTorch. The data set consists of a list of sentences, and our goal is to preprocess these sentences, tokenize them, and convert them into tensors of token indices for use in NLP models. Let's break down the code step by step.\n",
    "\n",
    "The sentences and the CustomDataset class are used in the same way as in the previous code snippet. The changes made to the CustomDataset class are as follows:\n",
    "\n",
    "- __init__: The constructor takes a list of sentences, a tokenizer function, and a vocabulary (vocab) as input.\n",
    "- __len__: This method returns the total number of samples in the data set.\n",
    "- __getitem__: This method is responsible for processing a single sample. It tokenizes the sentence using the provided tokenizer and then converts the tokens into tensor indices using the vocabulary.\n",
    "\n",
    "We can define a tokenizer using the `get_tokenizer` function with the `basic_english` option. Tokenization is the process of splitting a text into individual tokens or words. Next, we will build a vocabulary from the sentences. We will use the `build_vocab_from_iterator` function to construct the vocabulary from the tokenized sentences.\n",
    "\n",
    "We can create an instance of our custom dataset, passing in the sentences, tokenizer, and vocabulary. Finally, we will print the length of the custom data set and sample items from the data set for illustration.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8e3bcfac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Custom Dataset Length:  6\n",
      "Sample Items:\n",
      "Item 1: tensor([11, 19, 63, 17, 13,  2,  3, 47,  6, 16, 45,  0, 55,  3, 41, 46, 24, 10,\n",
      "        43, 61,  9, 44,  0, 14,  9, 33,  1])\n",
      "Item 2: tensor([35,  6, 16,  3, 38, 40,  0,  8,  1])\n",
      "Item 3: tensor([12,  5, 15, 31,  0,  8,  0, 57, 53,  2, 18, 62,  4,  0, 36, 49, 56, 15,\n",
      "        21,  1])\n",
      "Item 4: tensor([54, 18, 50, 23, 34, 58, 30, 27,  2,  5, 52,  7,  2,  5, 32,  1])\n",
      "Item 5: tensor([66, 29, 14, 13, 10, 22, 60,  7, 37,  1, 28, 51, 48,  4, 42, 11, 59, 39,\n",
      "         2, 12, 64, 17, 26, 65,  1])\n",
      "Item 6: tensor([19,  4, 25, 20])\n"
     ]
    }
   ],
   "source": [
    "sentences = [\n",
    "    \"If you want to know what a man's like, take a good look at how he treats his inferiors, not his equals.\",\n",
    "    \"Fame's a fickle friend, Harry.\",\n",
    "    \"It is our choices, Harry, that show what we truly are, far more than our abilities.\",\n",
    "    \"Soon we must all face the choice between what is right and what is easy.\",\n",
    "    \"Youth can not know how age thinks and feels. But old men are guilty if they forget what it was to be young.\",\n",
    "    \"You are awesome!\"\n",
    "]\n",
    "\n",
    "# Define a custom dataset\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, sentences, tokenizer, vocab):\n",
    "        self.sentences = sentences \n",
    "        self.tokenizer = tokenizer \n",
    "        self.vocab = vocab \n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sentences)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        tokens = self.tokenizer(self.sentences[idx])\n",
    "        # Convert tokens to tensor indices using vocab \n",
    "        tensor_indices = [self.vocab[token] for token in tokens]\n",
    "        return torch.tensor(tensor_indices)\n",
    "\n",
    "# Tokenizer \n",
    "tokenizer = get_tokenizer(\"basic_english\")\n",
    "\n",
    "# Build vocabulary \n",
    "# This is what turns words into numbers. \n",
    "vocab = build_vocab_from_iterator(map(tokenizer, sentences))\n",
    "# The numbers are assigned in order, but since we shuffled the dataset, the order looks random.\n",
    "\n",
    "# Create an instance of our custom data set \n",
    "custom_dataset = CustomDataset(sentences, tokenizer, vocab)\n",
    "\n",
    "print(\"Custom Dataset Length: \", len(custom_dataset))\n",
    "print(\"Sample Items:\")\n",
    "for i in range(6):\n",
    "    sample_item = custom_dataset[i]\n",
    "    print(f\"Item {i + 1}: {sample_item}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d39e1e68",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "stack expects each tensor to be equal size, but got [4] at entry 0 and [9] at entry 1",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[19], line 11\u001b[0m\n\u001b[0;32m      8\u001b[0m dataloader \u001b[38;5;241m=\u001b[39m DataLoader(custom_dataset, batch_size\u001b[38;5;241m=\u001b[39mbatch_size, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m     10\u001b[0m \u001b[38;5;66;03m# Iterate through the data loader\u001b[39;00m\n\u001b[1;32m---> 11\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m batch \u001b[38;5;129;01min\u001b[39;00m dataloader:\n\u001b[0;32m     12\u001b[0m     \u001b[38;5;28mprint\u001b[39m(batch)\n",
      "File \u001b[1;32mc:\\Users\\sekai\\.conda\\envs\\myenv\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:628\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    625\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    626\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    627\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 628\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    629\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    630\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    631\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    632\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[1;32mc:\\Users\\sekai\\.conda\\envs\\myenv\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:671\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    669\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    670\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 671\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    672\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[0;32m    673\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[1;32mc:\\Users\\sekai\\.conda\\envs\\myenv\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:61\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     59\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     60\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n\u001b[1;32m---> 61\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollate_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\sekai\\.conda\\envs\\myenv\\lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py:265\u001b[0m, in \u001b[0;36mdefault_collate\u001b[1;34m(batch)\u001b[0m\n\u001b[0;32m    204\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdefault_collate\u001b[39m(batch):\n\u001b[0;32m    205\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    206\u001b[0m \u001b[38;5;124;03m        Function that takes in a batch of data and puts the elements within the batch\u001b[39;00m\n\u001b[0;32m    207\u001b[0m \u001b[38;5;124;03m        into a tensor with an additional outer dimension - batch size. The exact output type can be\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    263\u001b[0m \u001b[38;5;124;03m            >>> default_collate(batch)  # Handle `CustomType` automatically\u001b[39;00m\n\u001b[0;32m    264\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 265\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcollate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcollate_fn_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdefault_collate_fn_map\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\sekai\\.conda\\envs\\myenv\\lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py:120\u001b[0m, in \u001b[0;36mcollate\u001b[1;34m(batch, collate_fn_map)\u001b[0m\n\u001b[0;32m    118\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m collate_fn_map \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    119\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m elem_type \u001b[38;5;129;01min\u001b[39;00m collate_fn_map:\n\u001b[1;32m--> 120\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcollate_fn_map\u001b[49m\u001b[43m[\u001b[49m\u001b[43melem_type\u001b[49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcollate_fn_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcollate_fn_map\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    122\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m collate_type \u001b[38;5;129;01min\u001b[39;00m collate_fn_map:\n\u001b[0;32m    123\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(elem, collate_type):\n",
      "File \u001b[1;32mc:\\Users\\sekai\\.conda\\envs\\myenv\\lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py:163\u001b[0m, in \u001b[0;36mcollate_tensor_fn\u001b[1;34m(batch, collate_fn_map)\u001b[0m\n\u001b[0;32m    161\u001b[0m     storage \u001b[38;5;241m=\u001b[39m elem\u001b[38;5;241m.\u001b[39mstorage()\u001b[38;5;241m.\u001b[39m_new_shared(numel, device\u001b[38;5;241m=\u001b[39melem\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[0;32m    162\u001b[0m     out \u001b[38;5;241m=\u001b[39m elem\u001b[38;5;241m.\u001b[39mnew(storage)\u001b[38;5;241m.\u001b[39mresize_(\u001b[38;5;28mlen\u001b[39m(batch), \u001b[38;5;241m*\u001b[39m\u001b[38;5;28mlist\u001b[39m(elem\u001b[38;5;241m.\u001b[39msize()))\n\u001b[1;32m--> 163\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstack\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mout\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: stack expects each tensor to be equal size, but got [4] at entry 0 and [9] at entry 1"
     ]
    }
   ],
   "source": [
    "\n",
    "# Create an instance of your custom data set\n",
    "custom_dataset = CustomDataset(sentences, tokenizer, vocab)\n",
    "\n",
    "# Define batch size\n",
    "batch_size = 2\n",
    "\n",
    "# Create a data loader\n",
    "dataloader = DataLoader(custom_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# Iterate through the data loader\n",
    "for batch in dataloader:\n",
    "    print(batch)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5e3beae",
   "metadata": {},
   "source": [
    "We will encounter an error when attempting to create batches for the tensors. This error arises because the tensor batches have unequal lengths. The data loader is using the default `collate_function`, which requires tensors to have equal lengths. We can define our own `collate_function` and pass the data into it to establish our own rules. Typically, to address the issue of unequal tensor lengths, we will employ data padding. This will be demonstrated in the following section.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15b8aec6",
   "metadata": {},
   "source": [
    "### Custom collate function\n",
    "\n",
    "A collate function is employed in the context of data loading and batching in machine learning, particularly when dealing with variable-length data, such as sequences (e.g., text, time series, sequences of events). Its primary purpose is to prepare and format individual data samples (examples) into batches that can be efficiently processed by machine learning models.\n",
    "\n",
    "We will begin by defining a custom collate function named `collate_fn`. This function plays a crucial role when handling sequences of varying lengths, such as sentences in NLP. Its purpose is to pad sequences within a batch to have equal lengths, which is a common preprocessing step in NLP tasks.\n",
    "\n",
    "`pad_sequence`: This function is a part of PyTorch and is utilized to pad sequences in a batch, ensuring uniform length. It takes a batch of sequences as input and pads them to match the length of the longest sequence. The `padding_value=0` argument specifies the value to use for padding.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ed555fac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a custom collate function \n",
    "def collate_fn(batch):\n",
    "    # Pad sequences within the batch to have equal lengths \n",
    "    padded_batch = pad_sequence(batch, batch_first=True, padding_value=0)\n",
    "    return padded_batch\n",
    "\n",
    "# Aligning sequence lengths."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9754db50",
   "metadata": {},
   "source": [
    "In the above cell, when padding the sequences, we set `batch_first=True`. When `batch_first=True`, output will be in [batch_size x seq_len] shape, otherwise it will be in [seq_len x batch_size] shape. Some models accept input with [batch_size x seq_len] shape while some other models need the input to be of [seq_len x batch_size] shape. Note that this parameter takes care of putting the input in the desired shape. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8202aeb",
   "metadata": {},
   "source": [
    "Let us see how it actually affects the shape of curated batches. First, we create a data loader fom a collate function with `batch_first = True`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "78580ca8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['if', 'you', 'want', 'to', 'know', 'what', 'a', 'man', \"'\", 's', 'like', ',', 'take', 'a', 'good', 'look', 'at', 'how', 'he', 'treats', 'his', 'inferiors', ',', 'not', 'his', 'equals', '.']\n",
      "['fame', \"'\", 's', 'a', 'fickle', 'friend', ',', 'harry', '.', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',']\n",
      "['it', 'is', 'our', 'choices', ',', 'harry', ',', 'that', 'show', 'what', 'we', 'truly', 'are', ',', 'far', 'more', 'than', 'our', 'abilities', '.']\n",
      "['soon', 'we', 'must', 'all', 'face', 'the', 'choice', 'between', 'what', 'is', 'right', 'and', 'what', 'is', 'easy', '.', ',', ',', ',', ',']\n",
      "['youth', 'can', 'not', 'know', 'how', 'age', 'thinks', 'and', 'feels', '.', 'but', 'old', 'men', 'are', 'guilty', 'if', 'they', 'forget', 'what', 'it', 'was', 'to', 'be', 'young', '.']\n",
      "['you', 'are', 'awesome', '!', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',']\n"
     ]
    }
   ],
   "source": [
    "# Create a data loader with the custom collate function with batch_first = True\n",
    "dataloader = DataLoader(custom_dataset, batch_size=batch_size, collate_fn = collate_fn)\n",
    "\n",
    "# Iterate through the data loader \n",
    "for batch in dataloader:\n",
    "    for row in batch:\n",
    "        for idx in row:\n",
    "            words = [vocab.get_itos()[idx] for idx in row]\n",
    "        print(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92f0c3ba",
   "metadata": {},
   "source": [
    "Looking into the result, we can see that the first dimension is the batch. For example, first batch is the first sentence. \"['if', 'you', 'want', 'to', 'know', 'what', 'a', 'man', \"'\", 's', 'like', ',', 'take', 'a', 'good', 'look', 'at', 'how', 'he', 'treats', 'his', 'inferiors', ',', 'not', 'his', 'equals', '.']\". "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39b202dd",
   "metadata": {},
   "source": [
    "Now, we can try `batch_first = False` which is the DEFAULT value:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "53192e82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a custom collate function\n",
    "def collate_fn_bfFALSE(batch):\n",
    "    # Pad sequences within the batch to have equal lengths \n",
    "    padded_batch = pad_sequence(batch, padding_value=0)\n",
    "    return padded_batch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d30146f2",
   "metadata": {},
   "source": [
    "Now, we look into the curated data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "fd62f118",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['if', 'fame']\n",
      "['you', \"'\"]\n",
      "['want', 's']\n",
      "['to', 'a']\n",
      "['know', 'fickle']\n",
      "['what', 'friend']\n",
      "['a', ',']\n",
      "['man', 'harry']\n",
      "[\"'\", '.']\n",
      "['s', ',']\n",
      "['like', ',']\n",
      "[',', ',']\n",
      "['take', ',']\n",
      "['a', ',']\n",
      "['good', ',']\n",
      "['look', ',']\n",
      "['at', ',']\n",
      "['how', ',']\n",
      "['he', ',']\n",
      "['treats', ',']\n",
      "['his', ',']\n",
      "['inferiors', ',']\n",
      "[',', ',']\n",
      "['not', ',']\n",
      "['his', ',']\n",
      "['equals', ',']\n",
      "['.', ',']\n",
      "['it', 'soon']\n",
      "['is', 'we']\n",
      "['our', 'must']\n",
      "['choices', 'all']\n",
      "[',', 'face']\n",
      "['harry', 'the']\n",
      "[',', 'choice']\n",
      "['that', 'between']\n",
      "['show', 'what']\n",
      "['what', 'is']\n",
      "['we', 'right']\n",
      "['truly', 'and']\n",
      "['are', 'what']\n",
      "[',', 'is']\n",
      "['far', 'easy']\n",
      "['more', '.']\n",
      "['than', ',']\n",
      "['our', ',']\n",
      "['abilities', ',']\n",
      "['.', ',']\n",
      "['youth', 'you']\n",
      "['can', 'are']\n",
      "['not', 'awesome']\n",
      "['know', '!']\n",
      "['how', ',']\n",
      "['age', ',']\n",
      "['thinks', ',']\n",
      "['and', ',']\n",
      "['feels', ',']\n",
      "['.', ',']\n",
      "['but', ',']\n",
      "['old', ',']\n",
      "['men', ',']\n",
      "['are', ',']\n",
      "['guilty', ',']\n",
      "['if', ',']\n",
      "['they', ',']\n",
      "['forget', ',']\n",
      "['what', ',']\n",
      "['it', ',']\n",
      "['was', ',']\n",
      "['to', ',']\n",
      "['be', ',']\n",
      "['young', ',']\n",
      "['.', ',']\n"
     ]
    }
   ],
   "source": [
    "# Create a data loader with the custom collate function with batch_first = True\n",
    "dataloader_bfFALSE = DataLoader(custom_dataset, batch_size=batch_size, collate_fn = collate_fn_bfFALSE)\n",
    "\n",
    "# Iterate through the data loader\n",
    "for seq in dataloader_bfFALSE:\n",
    "    for row in seq:\n",
    "        words = [vocab.get_itos()[idx] for idx in row]\n",
    "        print(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c219aea6",
   "metadata": {},
   "source": [
    "It can be seen that the first dimension is now the sequence instead of batch, which means sentences will break so that each row includes a token from each sequence. For example the first row ([\"if\", \"fame]), includes the first tokens of all the sequence in that batch. We need to be aware of this standard to avoid any confusion when working with recurrent neural networks (RNNs) and transformers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f3dd2aaa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[11, 19, 63, 17, 13,  2,  3, 47,  6, 16, 45,  0, 55,  3, 41, 46, 24, 10,\n",
      "         43, 61,  9, 44,  0, 14,  9, 33,  1],\n",
      "        [35,  6, 16,  3, 38, 40,  0,  8,  1,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "          0,  0,  0,  0,  0,  0,  0,  0,  0]])\n",
      "Length of sequences in the batch:  27\n",
      "tensor([[12,  5, 15, 31,  0,  8,  0, 57, 53,  2, 18, 62,  4,  0, 36, 49, 56, 15,\n",
      "         21,  1],\n",
      "        [54, 18, 50, 23, 34, 58, 30, 27,  2,  5, 52,  7,  2,  5, 32,  1,  0,  0,\n",
      "          0,  0]])\n",
      "Length of sequences in the batch:  20\n",
      "tensor([[66, 29, 14, 13, 10, 22, 60,  7, 37,  1, 28, 51, 48,  4, 42, 11, 59, 39,\n",
      "          2, 12, 64, 17, 26, 65,  1],\n",
      "        [19,  4, 25, 20,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "          0,  0,  0,  0,  0,  0,  0]])\n",
      "Length of sequences in the batch:  25\n"
     ]
    }
   ],
   "source": [
    "# Iterate though the data loader with batch_first = TRUE\n",
    "for batch in dataloader:\n",
    "    print(batch)\n",
    "    print(\"Length of sequences in the batch: \", batch.shape[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cac1865",
   "metadata": {},
   "source": [
    "We will see that each batch has a fixed size for all sequences within the batch.\n",
    "\n",
    "We also have the option to utilize the collate function for tasks such as tokenization, converting tokenized indices, and transforming the result into a tensor. It is important to note that the original dataset remains untouched by these transformations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "4b1548bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a custom dataset\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, sentences):\n",
    "        self.sentences = sentences \n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sentences)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.sentences[idx]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "883304fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_dataset = CustomDataset(sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d4e521c",
   "metadata": {},
   "source": [
    "We have the raw text:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "be294fd8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"If you want to know what a man's like, take a good look at how he treats his inferiors, not his equals.\""
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "custom_dataset[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57b74bfa",
   "metadata": {},
   "source": [
    "We create the new `collate_fn`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "7dbbc337",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    # Tokenize each sample in the batch using the specified tokenizer \n",
    "    tensor_batch = []\n",
    "    for sample in batch:\n",
    "        tokens = tokenizer(sample)\n",
    "        # Convert tokens to vocabulary indices and create a tensor for each sample \n",
    "        tensor_batch.append(torch.tensor([vocab[token] for token in tokens]))\n",
    "\n",
    "    # Pad sequences within the batch to have equal lengths using pad_sequence\n",
    "    # batch_first = True ensures that the tensors have shape (batch_size, max_sequence_length)\n",
    "    padded_batch = pad_sequence(tensor_batch, batch_first=True)\n",
    "\n",
    "    # Return the padded batch \n",
    "    return padded_batch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e647d338",
   "metadata": {},
   "source": [
    "Create a data loader with the custom collate function "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "c9b1b5b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a data loader for the custom dataset \n",
    "dataloader = DataLoader(\n",
    "    dataset = custom_dataset,   # Custom PyTorch Dataset containing our data\n",
    "    batch_size = batch_size,    # Number of samples in each mini-batch \n",
    "    shuffle = True,             # Shuffle the data at the beginning of each epoch \n",
    "    collate_fn = collate_fn     # Custom collate function for processing batches\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78e27c2b",
   "metadata": {},
   "source": [
    "We will see that the result is a tensor of the shape for each sample in the batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "dfff75a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[35,  6, 16,  3, 38, 40,  0,  8,  1,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "          0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
      "        [11, 19, 63, 17, 13,  2,  3, 47,  6, 16, 45,  0, 55,  3, 41, 46, 24, 10,\n",
      "         43, 61,  9, 44,  0, 14,  9, 33,  1]])\n",
      "shape of sample:  2\n",
      "tensor([[54, 18, 50, 23, 34, 58, 30, 27,  2,  5, 52,  7,  2,  5, 32,  1],\n",
      "        [19,  4, 25, 20,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0]])\n",
      "shape of sample:  2\n",
      "tensor([[12,  5, 15, 31,  0,  8,  0, 57, 53,  2, 18, 62,  4,  0, 36, 49, 56, 15,\n",
      "         21,  1,  0,  0,  0,  0,  0],\n",
      "        [66, 29, 14, 13, 10, 22, 60,  7, 37,  1, 28, 51, 48,  4, 42, 11, 59, 39,\n",
      "          2, 12, 64, 17, 26, 65,  1]])\n",
      "shape of sample:  2\n"
     ]
    }
   ],
   "source": [
    "for batch in dataloader:\n",
    "    print(batch)\n",
    "    print(\"shape of sample: \", len(batch))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3be5b578",
   "metadata": {},
   "source": [
    "As a result, batches of tensors with equal lengths have been successfully created."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dc023d7",
   "metadata": {},
   "source": [
    "Creatinh a data loader with a collate function that processes batches of French text. Sort the data set on sequences length. Then tokenize, numericalize and pad the sequences. Sorting the sequences will minimize the number of `<PAD>`tokens added to the sequences, which enhances the model's performance. We will prepare the data in batches of size 4 and print them.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "4e3742ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = [\n",
    "    \"Ceci est une phrase.\",\n",
    "    \"C'est un autre exemple de phrase.\",\n",
    "    \"Voici une troisième phrase.\",\n",
    "    \"Il fait beau aujourd'hui.\",\n",
    "    \"J'aime beaucoup la cuisine française.\",\n",
    "    \"Quel est ton plat préféré ?\",\n",
    "    \"Je t'adore.\",\n",
    "    \"Bon appétit !\",\n",
    "    \"Je suis en train d'apprendre le français.\",\n",
    "    \"Nous devons partir tôt demain matin.\",\n",
    "    \"Je suis heureux.\",\n",
    "    \"Le film était vraiment captivant !\",\n",
    "    \"Je suis là.\",\n",
    "    \"Je ne sais pas.\",\n",
    "    \"Je suis fatigué après une longue journée de travail.\",\n",
    "    \"Est-ce que tu as des projets pour le week-end ?\",\n",
    "    \"Je vais chez le médecin cet après-midi.\",\n",
    "    \"La musique adoucit les mœurs.\",\n",
    "    \"Je dois acheter du pain et du lait.\",\n",
    "    \"Il y a beaucoup de monde dans cette ville.\",\n",
    "    \"Merci beaucoup !\",\n",
    "    \"Au revoir !\",\n",
    "    \"Je suis ravi de vous rencontrer enfin !\",\n",
    "    \"Les vacances sont toujours trop courtes.\",\n",
    "    \"Je suis en retard.\",\n",
    "    \"Félicitations pour ton nouveau travail !\",\n",
    "    \"Je suis désolé, je ne peux pas venir à la réunion.\",\n",
    "    \"À quelle heure est le prochain train ?\",\n",
    "    \"Bonjour !\",\n",
    "    \"C'est génial !\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "138d6c0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please install SpaCy. See the docs at https://spacy.io for more information.\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'spacy'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[53], line 12\u001b[0m\n\u001b[0;32m      9\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m padded_batch\n\u001b[0;32m     11\u001b[0m \u001b[38;5;66;03m# Build tokenizer\u001b[39;00m\n\u001b[1;32m---> 12\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m \u001b[43mget_tokenizer\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mspacy\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlanguage\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mfr_core_news_sm\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;66;03m# Build vocabulary\u001b[39;00m\n\u001b[0;32m     15\u001b[0m vocab \u001b[38;5;241m=\u001b[39m build_vocab_from_iterator(\u001b[38;5;28mmap\u001b[39m(tokenizer, corpus))\n",
      "File \u001b[1;32mc:\\Users\\sekai\\.conda\\envs\\myenv\\lib\\site-packages\\torchtext\\data\\utils.py:91\u001b[0m, in \u001b[0;36mget_tokenizer\u001b[1;34m(tokenizer, language)\u001b[0m\n\u001b[0;32m     89\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m tokenizer \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mspacy\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m     90\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 91\u001b[0m         \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mspacy\u001b[39;00m\n\u001b[0;32m     93\u001b[0m         \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     94\u001b[0m             spacy \u001b[38;5;241m=\u001b[39m spacy\u001b[38;5;241m.\u001b[39mload(language)\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'spacy'"
     ]
    }
   ],
   "source": [
    "def collate_fn_fr(batch):\n",
    "    # Pad sequences within the batch to have equal lengths\n",
    "    tensor_batch=[]\n",
    "    for sample in batch:\n",
    "        tokens = tokenizer(sample)\n",
    "        tensor_batch.append(torch.tensor([vocab[token] for token in tokens]))\n",
    "         \n",
    "    padded_batch = pad_sequence(tensor_batch,batch_first=True)\n",
    "    return padded_batch\n",
    "\n",
    "# Build tokenizer\n",
    "tokenizer = get_tokenizer('spacy', language='fr_core_news_sm')\n",
    "\n",
    "# Build vocabulary\n",
    "vocab = build_vocab_from_iterator(map(tokenizer, corpus))\n",
    "\n",
    "# Sort sentences based on their length\n",
    "sorted_data = sorted(corpus, key=lambda x: len(tokenizer(x)))\n",
    "#print(sorted_data)\n",
    "dataloader = DataLoader(sorted_data, batch_size=4, shuffle=False, collate_fn=collate_fn_fr)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
