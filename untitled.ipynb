{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "70198547",
   "metadata": {},
   "source": [
    "##  Building and Training a Feedforward Neural Network for Language Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3771dc6c",
   "metadata": {},
   "source": [
    "This project explores the use of Feedforward Neural Networks (FNNs) in language modeling. The primary objective is to build a neural network that learns word relationships and generates meaningful text sequences. The implementation is done using PyTorch, covering key aspects of Natural Language Processing (NLP), such as:\n",
    "* Tokenization & Indexing: Converting text into numerical representations.\n",
    "* Embedding Layers: Mapping words to dense vector representations for efficient learning.\n",
    "* Context-Target Pair Generation (N-grams): Structuring training data for sequence prediction.\n",
    "* Multi-Class Neural Network: Designing a model to predict the next word in a sequence.\n",
    "\n",
    "The training process includes optimizing the model with loss functions and backpropagation techniques to improve accuracy and coherence in text generation. By the end of the project, we will have a working FNN-based language model capable of generating text sequences."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "014f57fb",
   "metadata": {},
   "source": [
    " - Implement a feedforward neural network using the PyTorch framework, including embedding layers, for language modeling tasks.\n",
    " - Fine-tune the output layer of the neural network for optimal performance in text generation.\n",
    " - Apply various training strategies and fundamental Natural Language Processing (NLP) techniques, such as tokenization and sequence analysis, to improve text generation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b9b48c1",
   "metadata": {},
   "source": [
    "### Importing Requierd Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e13a0d73",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\sekai\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\sekai\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "from tqdm import tqdm\n",
    "\n",
    "warnings.simplefilter('ignore')\n",
    "import time\n",
    "from collections import OrderedDict\n",
    "\n",
    "import re\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import random\n",
    "\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('punkt_tab')\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import string\n",
    "import time\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "# We can also use this section to suppress warnings generated by our code:\n",
    "def warn(*args, **kwargs):\n",
    "    pass\n",
    "import warnings\n",
    "warnings.warn = warn\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f65b8211",
   "metadata": {},
   "source": [
    "### Feedforward Neural Network (FNNs) for Language Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8130696e",
   "metadata": {},
   "source": [
    "FNNs, or Multi-Layer Perceptrons, serve as the foundational components for comprehending neural networks in natural language processing (NLP). In NLP tasks, FNNs process textual data by transforming it into numerical vectors known as embeddings. Subsequently, these embeddings are input to the network to predict language facets, such as the upcoming word in a sentence or the sentiment of a text.\n",
    "\n",
    "Let us consider the following song lyrics for our analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b22aeed3",
   "metadata": {},
   "outputs": [],
   "source": [
    "song= \"\"\"We are no strangers to love\n",
    "You know the rules and so do I\n",
    "A full commitments what Im thinking of\n",
    "You wouldnt get this from any other guy\n",
    "I just wanna tell you how Im feeling\n",
    "Gotta make you understand\n",
    "Never gonna give you up\n",
    "Never gonna let you down\n",
    "Never gonna run around and desert you\n",
    "Never gonna make you cry\n",
    "Never gonna say goodbye\n",
    "Never gonna tell a lie and hurt you\n",
    "Weve known each other for so long\n",
    "Your hearts been aching but youre too shy to say it\n",
    "Inside we both know whats been going on\n",
    "We know the game and were gonna play it\n",
    "And if you ask me how Im feeling\n",
    "Dont tell me youre too blind to see\n",
    "Never gonna give you up\n",
    "Never gonna let you down\n",
    "Never gonna run around and desert you\n",
    "Never gonna make you cry\n",
    "Never gonna say goodbye\n",
    "Never gonna tell a lie and hurt you\n",
    "Never gonna give you up\n",
    "Never gonna let you down\n",
    "Never gonna run around and desert you\n",
    "Never gonna make you cry\n",
    "Never gonna say goodbye\n",
    "Never gonna tell a lie and hurt you\n",
    "Weve known each other for so long\n",
    "Your hearts been aching but youre too shy to say it\n",
    "Inside we both know whats been going on\n",
    "We know the game and were gonna play it\n",
    "I just wanna tell you how Im feeling\n",
    "Gotta make you understand\n",
    "Never gonna give you up\n",
    "Never gonna let you down\n",
    "Never gonna run around and desert you\n",
    "Never gonna make you cry\n",
    "Never gonna say goodbye\n",
    "Never gonna tell a lie and hurt you\n",
    "Never gonna give you up\n",
    "Never gonna let you down\n",
    "Never gonna run around and desert you\n",
    "Never gonna make you cry\n",
    "Never gonna say goodbye\n",
    "Never gonna tell a lie and hurt you\n",
    "Never gonna give you up\n",
    "Never gonna let you down\n",
    "Never gonna run around and desert you\n",
    "Never gonna make you cry\n",
    "Never gonna say goodbye\n",
    "Never gonna tell a lie and hurt you\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "249250a4",
   "metadata": {},
   "source": [
    "### Tokenization for FNN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "103de616",
   "metadata": {},
   "source": [
    "This PyTorch function is used to obtain a tokenizer for text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "f25c5fef",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = get_tokenizer(\"basic_english\")\n",
    "tokens = tokenizer(song)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "d8c6ee22",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_string(s):\n",
    "    \"\"\"\n",
    "    Preprocesses a given string by performing the following steps:\n",
    "    \n",
    "    1. Removes all non-word characters (excluding letters and numbers).\n",
    "    2. Removes all whitespace characters.\n",
    "    3. Removes all numeric digits.\n",
    "\n",
    "    Parameters:\n",
    "    s (str): The input string to be cleaned.\n",
    "\n",
    "    Returns:\n",
    "    str: The processed string with only alphabetic characters, no spaces, and no digits.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Remove all non-word characters (everything except letters and numbers)\n",
    "    # \\w matches any word character (letters, numbers, and underscores)\n",
    "    # \\s matches any whitespace characters\n",
    "    # ^ inside [] negates the selection, so [^\\w\\s] matches anything that's NOT a word character or whitespace.\n",
    "    s = re.sub(r\"[^\\w\\s]\", '', s)\n",
    "\n",
    "    # Remove all whitespace characters (spaces, tabs, newlines)\n",
    "    # \\s+ matches one or more whitespace characters.\n",
    "    s = re.sub(r\"\\s+\", '', s)\n",
    "\n",
    "    # Remove all digits (0-9)\n",
    "    # \\d matches any digit character.\n",
    "    s = re.sub(r\"\\d\", '', s)\n",
    "\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "fb378541",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(words):\n",
    "    \"\"\"\n",
    "    Preprocesses a given text by tokenizing it, cleaning individual words, and \n",
    "    converting them to lowercase while removing empty or punctuation tokens.\n",
    "\n",
    "    Steps:\n",
    "    1. Tokenization: Splits the input text into individual word tokens.\n",
    "    2. Cleaning: Applies `preprocess_string()` to remove non-word characters, \n",
    "       spaces, and digits from each token.\n",
    "    3. Normalization: Converts all tokens to lowercase.\n",
    "    4. Filtering: Removes empty strings and punctuation tokens.\n",
    "\n",
    "    Parameters:\n",
    "    words (str): The input text to be tokenized and preprocessed.\n",
    "\n",
    "    Returns:\n",
    "    list: A list of cleaned, lowercase tokens.\n",
    "    \"\"\"\n",
    "\n",
    "    # Tokenize the input text into words \n",
    "    tokens = word_tokenize(words)\n",
    "\n",
    "    # Apply preprocessing to each token (removes unwanted characters)\n",
    "    tokens = [preprocess_string(w) for w in tokens]\n",
    "\n",
    "    # Convert tokens to lowercase and remoce empty strings or punctuation \n",
    "    return [w.lower() for w in tokens if len(w) != 0 and w not in string.punctuation]\n",
    "\n",
    "# Example usage:\n",
    "tokens = preprocess(song) # Preprocess the text in 'song'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7177e62b",
   "metadata": {},
   "source": [
    "### Indexing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68447533",
   "metadata": {},
   "source": [
    "TorchText provides tools to tokenize text into individual words (tokens) and build a vocabulary, which maps tokens to unique integer indexes. This is a crucial step in preparing text data for  machine learning models that require numerical input.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "a89896e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenizetext(song):\n",
    "    \"\"\"\n",
    "    Tokenizes the input text (song) and builds a vocabulary from the tokens.\n",
    "\n",
    "    Steps:\n",
    "    1. Tokenization: The function splits the input text into words and applies \n",
    "       a tokenizer function to each word.\n",
    "    2. Vocabulary Building: Constructs a vocabulary from the tokenized words,\n",
    "       including a special \"<unk>\" token to handle out-of-vocabulary words.\n",
    "    3. Default Indexing: Sets the default index for unknown words, ensuring \n",
    "       that any unseen tokens are mapped to \"<unk>\".\n",
    "\n",
    "    Parameters:\n",
    "    song (str): The input text (song lyrics) to be tokenized and processed.\n",
    "\n",
    "    Returns:\n",
    "    vocab (Vocab): A vocabulary object mapping tokens to their corresponding indices.\n",
    "    \"\"\"\n",
    "\n",
    "    # Tokenize the text\n",
    "    # Split the input text into words and apply the tokenizer function to each word.\n",
    "    # The 'map' function ensures that each word is tokenized properly. \n",
    "    tokenized_song = map(tokenizer, song.split())\n",
    "\n",
    "    # Build vocabulary from tokenized text\n",
    "    # The function `build_vocab_from_iterator` constructs a vocabulary by iterating \n",
    "    # over the tokenized words. The special token \"<unk>\" is added to handle words \n",
    "    # that are not present in the vocabulary.\n",
    "    vocab = build_vocab_from_iterator(tokenized_song, specials=[\"<unk>\"])\n",
    "\n",
    "    # Set the default index for unknown words\n",
    "    # The default index is set to the index of \"<unk>\" so that any word not found \n",
    "    # in the vocabulary is mapped to this token, preventing errors during lookup.\n",
    "    vocab.set_default_index(vocab[\"<unk>\"])\n",
    "\n",
    "    return vocab "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dde1df1d",
   "metadata": {},
   "source": [
    "Convert the tokens to indices by applying the function as shown here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "038c0ea2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[21, 58, 70, 74, 25, 69, 2, 20, 31, 72]"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab = tokenizetext(song)\n",
    "vocab(tokens[0:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "ddaa5bb1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['we', 'are', 'no', 'strangers', 'to', 'love', 'you', 'know', 'the', 'rules']"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5546e47d",
   "metadata": {},
   "source": [
    "We will write a text function that converts raw text into indices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "7568695d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[21, 58, 70, 74, 25, 69, 2, 20, 31, 72]"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_pipeline = lambda x: vocab(tokenizer(x))\n",
    "text_pipeline(song)[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9bf78ce",
   "metadata": {},
   "source": [
    "Find the word corresponsing to an index using the `get_itos()` method. The result is a list where the index of the list corresponds to a word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "ee9db3c2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'are'"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index_to_token = vocab.get_itos()\n",
    "index_to_token[58]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e8283c1",
   "metadata": {},
   "source": [
    "### Embedding Layers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7adff660",
   "metadata": {},
   "source": [
    "An embedding layer is a crucial element in natural language processing (NLP) and neural networks designed for sequential data. It serves to convert categorical variables, like words or discrete indexes representing tokens, into continuous vectors. This transformation facilitates training and enables the network to learn meaningful relationships among words.\n",
    "\n",
    "Let's consider a simple example involving a vocabulary of words \n",
    "- **Vocabulary**: {apple, banana, orange, pear}\n",
    "\n",
    "Each word in our vocabulary has a unique index assigned to it: \n",
    "- **Indices**: {0, 1, 2, 3}\n",
    "\n",
    "When using an embedding layer, we will initialize random continuous vectors for each index. For instance, the embedding vectors might look like:\n",
    "\n",
    "- Vector for index 0 (apple): [0.2, 0.8]\n",
    "- Vector for index 1 (banana): [0.6, -0.5]\n",
    "- Vector for index 2 (orange): [-0.3, 0.7]\n",
    "- Vector for index 3 (pear): [0.1, 0.4]\n",
    "In PyTorch, we can create an embedding layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "ed367b41",
   "metadata": {},
   "outputs": [],
   "source": [
    "def genembedding(vocab):\n",
    "    \"\"\"\n",
    "    Generates an embedding layer for the given vocabulary.\n",
    "\n",
    "    The embedding layer transforms words into dense vector representations, \n",
    "    allowing the model to learn semantic relationships between words.\n",
    "\n",
    "    Parameters:\n",
    "    vocab (Vocab): The vocabulary object containing unique words and their indices.\n",
    "\n",
    "    Returns:\n",
    "    nn.Embedding: A PyTorch embedding layer with a specified embedding dimension.\n",
    "    \"\"\"\n",
    "\n",
    "    # Define the embedding dimension (size of word vectors)\n",
    "    embedding_dim = 20 \n",
    "\n",
    "    # Get the vocabulary size (number of unique words in the vocabulary)\n",
    "    vocab_size = len(vocab) \n",
    "\n",
    "    # Create an embedding layer \n",
    "    # The nn.Embedding module maps word indices to dense vector representations. \n",
    "    # It takes vocab_size as the number of words and embedding_dim as the vector size.\n",
    "\n",
    "    embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
    "\n",
    "    return embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3748d670",
   "metadata": {},
   "source": [
    "**Embeddings**: Obtain the embedding for the first word with index 0 or 1. Don't forget that we have to convert the input into a tensor. The embeddings are initially initialized randomly, but as the model undergoes training, words with similar meanings gradually come to cluster closer together\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "4cb0fb1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word <unk>\n",
      "index 0\n",
      "embedding tensor([-0.1948, -0.0872, -1.1712, -2.4286, -0.6913,  1.3402, -0.6221,  0.4498,\n",
      "        -0.9831,  1.2930,  0.8697, -0.1732,  2.9601,  0.9453,  0.1231, -0.5225,\n",
      "         0.0386,  0.0675,  2.6692,  0.7498], grad_fn=<EmbeddingBackward0>)\n",
      "embedding shape torch.Size([20])\n",
      "word gonna\n",
      "index 1\n",
      "embedding tensor([ 0.2825,  0.3624,  1.6737,  1.6189, -1.8306, -1.2440, -0.3699,  0.8209,\n",
      "         0.4144,  1.3435, -0.0302, -0.6240, -1.0160,  1.4343,  0.3890, -0.4814,\n",
      "        -0.9842,  0.2842, -1.3067, -0.9070], grad_fn=<EmbeddingBackward0>)\n",
      "embedding shape torch.Size([20])\n"
     ]
    }
   ],
   "source": [
    "embeddings = genembedding(vocab)\n",
    "\n",
    "for n in range(2):\n",
    "    embedding = embeddings(torch.tensor(n))\n",
    "    print(\"word\", index_to_token[n])\n",
    "    print(\"index\", n)\n",
    "    print(\"embedding\", embedding)\n",
    "    print(\"embedding shape\", embedding.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3849430",
   "metadata": {},
   "source": [
    "These vectors will serve as inputs for the next layer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2e96aed",
   "metadata": {},
   "source": [
    "### Generating Context-Target Pairs (n-grams)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72fff67e",
   "metadata": {},
   "source": [
    "Organize words within a variable-size context using the following approach: Each word is denoted by 'i'. \n",
    "To establish the context, simply subtract 'j'. The size of the context is determined by the value of ``CONTEXT_SIZE``."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "ce92e858",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the context size for generating n-grams\n",
    "CONTEXT_SIZE = 2 # The number of previous words used to predict the next word \n",
    "\n",
    "def genngrams(tokens):\n",
    "    \"\"\"\n",
    "    Generates n-grams from a list of tokens, where each n-gram consists of a \n",
    "    context (previous words) and a target (next word).\n",
    "\n",
    "    The function constructs a list of tuples where:\n",
    "    - The first element is a list of `CONTEXT_SIZE` previous words.\n",
    "    - The second element is the target word that follows the context.\n",
    "\n",
    "    Parameters:\n",
    "    tokens (list): A list of preprocessed word tokens.\n",
    "\n",
    "    Returns:\n",
    "    list: A list of tuples representing n-grams.\n",
    "          Each tuple contains (context_words, target_word).\n",
    "    \"\"\"\n",
    "\n",
    "    # Generate n-grams \n",
    "    # Iterate through the tokens starting from index CONTEXT_SIZR to the end \n",
    "    # For each token at posisiton 'i', extract the previous CONTEXT_SIZE words as context \n",
    "    ngrams = [\n",
    "        (\n",
    "            [tokens [i - j - 1] for j in range(CONTEXT_SIZE)], # Context words (previous words)\n",
    "            tokens[i] # Target word (the word to predict)\n",
    "        )\n",
    "        for i in range(CONTEXT_SIZE, len(tokens))\n",
    "    ]\n",
    "\n",
    "    return ngrams\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea29d027",
   "metadata": {},
   "source": [
    "Output the first element, which results in a tuple. The initial element represents the context, and the index indicates the following word. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "8afe5122",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "context ['are', 'we'] target no\n",
      "context inex [58, 21] target index [70]\n"
     ]
    }
   ],
   "source": [
    "ngrams = genngrams(tokens)\n",
    "context, target = ngrams[0]\n",
    "print(\"context\", context, \"target\", target)\n",
    "print(\"context inex\", vocab(context), \"target index\", vocab([target]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b3ca245",
   "metadata": {},
   "source": [
    "In this context, there are multiple words. Aggregate the embeddings of each of these words and then adjust the input size of the subsequent layer accordingly. Then, create the next layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "666b04f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dim = 20\n",
    "linear = nn.Linear(embedding_dim*CONTEXT_SIZE, 128)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15291963",
   "metadata": {},
   "source": [
    "We have the two embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "22c8dbb7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 20])"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings = genembedding(vocab)\n",
    "my_embeddings = embeddings(torch.tensor(vocab(context)))\n",
    "my_embeddings.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d10fe9a3",
   "metadata": {},
   "source": [
    "Reshape the embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "df297701",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 40])"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_embeddings = my_embeddings.reshape(1, -1)\n",
    "my_embeddings.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b2b43bc",
   "metadata": {},
   "source": [
    "They can now be used as inputs in the next layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "a882ddea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.4028, -1.4974,  0.4740,  0.1218, -0.5506,  0.1006,  0.1310, -0.2432,\n",
       "          0.0517, -0.7850,  0.3461, -0.0670,  0.5225,  0.4835,  1.0080,  0.6244,\n",
       "          0.8042,  0.0659,  0.2579,  0.4803,  0.3325, -0.1497,  0.4614,  0.0488,\n",
       "          0.2879, -0.0150, -0.3574,  1.1154,  0.8951,  1.0535,  0.3276, -0.2760,\n",
       "          0.4410, -0.2273,  0.8928,  0.4261,  0.3102, -0.1316, -0.1292, -0.1616,\n",
       "         -0.1769,  0.0392, -0.5153,  0.8410, -0.2673, -0.0222, -0.6228, -0.0354,\n",
       "          0.7729, -0.5941, -0.0075,  0.1556,  0.8108, -0.2758,  0.0297,  1.0555,\n",
       "          0.4652,  0.6695, -0.4220,  0.6633,  0.1155, -0.5431,  0.1106,  0.1090,\n",
       "         -0.2540, -0.2349,  0.9288,  0.2240,  0.2332, -0.4005, -0.7519,  0.0186,\n",
       "         -0.4358,  0.5097, -0.5526, -0.6808,  0.5598, -0.0052, -0.2793, -0.7719,\n",
       "         -0.6207, -1.0954,  0.6852,  0.5970, -1.4211,  0.2467, -0.4849,  0.3938,\n",
       "          0.1085, -0.8326, -0.3311, -0.7827, -0.1324,  0.0725, -0.1618,  0.1830,\n",
       "          0.5206, -0.1482, -0.2154,  1.6144,  0.5799, -0.4984, -0.3455, -0.5140,\n",
       "         -0.1017,  0.1046,  0.1389, -0.2402,  0.7287, -0.5372, -0.2567,  0.2053,\n",
       "         -0.0793,  0.1232, -0.7348, -0.5005,  0.0101,  0.0459,  0.4933, -1.1090,\n",
       "         -0.8421,  0.2952,  0.4755, -0.8604,  0.7463, -0.2910,  0.9518, -0.3310]],\n",
       "       grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "linear(my_embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2aecdfe6",
   "metadata": {},
   "source": [
    "### Batch Function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33d22a7b",
   "metadata": {},
   "source": [
    "Create a Batch function to interface with the data loader. Several adjustments are necessary to handle words that are part of a context in one batch and a predicted word in the following batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "70d2f7fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader \n",
    "import torch\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "CONTEXT_SIZE = 3    # Number of previous words used as context for prediction\n",
    "BATCH_SIZE = 10     # Number of samples per training batch\n",
    "EMBEDDING_DIM = 10  # Dimension of word embeddings\n",
    "\n",
    "def collate_batch(batch):\n",
    "    \"\"\"\n",
    "    Processes a batch of text data into input (context) and output (target) tensors\n",
    "    for training a language model.\n",
    "\n",
    "    The function extracts:\n",
    "    - `context`: A list of word indices representing the context words for each target word.\n",
    "    - `target`: A list of word indices representing the target word to predict.\n",
    "\n",
    "    Parameters:\n",
    "    batch (list): A list of tokenized words (strings).\n",
    "\n",
    "    Returns:\n",
    "    tuple: Two PyTorch tensors: (context_tensor, target_tensor)\n",
    "           - context_tensor: Tensor of shape (batch_size - CONTEXT_SIZE, CONTEXT_SIZE),\n",
    "             containing the word indices of context words.\n",
    "           - target_tensor: Tensor of shape (batch_size - CONTEXT_SIZE,),\n",
    "             containing the word indices of target words.\n",
    "    \"\"\"\n",
    "\n",
    "    batch_size = len(batch)\n",
    "    context, target = [], []\n",
    "\n",
    "    # Loop through the batch, ensuring enough previous words exist for context \n",
    "    for i in range(CONTEXT_SIZE, batch_size):\n",
    "        # Convert the target word to its index using the vocabulary \n",
    "        target.append(vocab([batch[i]]))\n",
    "\n",
    "        # Convert the previous CONTEXT_SIZE words to indices using the vocabulary \n",
    "        context.append(vocab([batch[i - j - 1] for j in range(CONTEXT_SIZE)]))\n",
    "\n",
    "    # Convert lists to PyTorch tensors and move them to the appropriate device (CPU/GPU)\n",
    "    return torch.tensor(context).to(device), torch.tensor(target).to(device).reshape(-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e066fab0",
   "metadata": {},
   "source": [
    "Similarly, it's important to highlight that the size of the last batch could deviate from that of the earlier batches. To tackle this, the approach involves adjusting the final batch to conform to the specified batch size, ensuring it becomes a multiple of the predetermined size. When necessary, we'll employ padding techniques to achieve this harmonization. One approach we'll use is appending the beginning of the song to the end of the batch.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "837480f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "Padding = BATCH_SIZE - len(tokens) % BATCH_SIZE\n",
    "tokens_pad = tokens + tokens[0:Padding]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f4d7946",
   "metadata": {},
   "source": [
    "Create the `DataLoader`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "8cfa66bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader = DataLoader(\n",
    "    tokens_pad, batch_size = BATCH_SIZE, shuffle = False, collate_fn = collate_batch\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe4e7433",
   "metadata": {},
   "source": [
    "### Multi-Class Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe03c623",
   "metadata": {},
   "source": [
    "We have developed a PyTorch class for a multi-class neural network. The network's output is the probability of the next word within a given context. Therefore, the number of classes corresponds to the count of distinct words. The initial layer consists of embeddings, and in addition to the final later, an extra hidden layer is incorporated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "e38e8200",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NGramLanguageModeler(nn.Module):\n",
    "    \"\"\"\n",
    "    A neural network-based n-gram language model that predicts the next word \n",
    "    given a sequence of context words.\n",
    "\n",
    "    This model consists of:\n",
    "    - An embedding layer that converts word indices into dense vector representations.\n",
    "    - A fully connected hidden layer with ReLU activation.\n",
    "    - An output layer that predicts the probability distribution over the vocabulary.\n",
    "\n",
    "    Parameters:\n",
    "    vocab_size (int): The number of unique words in the vocabulary.\n",
    "    embedding_dim (int): The size of the word embeddings (vector representation of words).\n",
    "    context_size (int): The number of previous words used as context to predict the next word.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, vocab_size, embedding_dim, context_size):\n",
    "        super(NGramLanguageModeler, self).__init__()\n",
    "        \n",
    "        # Store context size and embedding dimension \n",
    "        self.context_size = context_size \n",
    "        self.embedding_dim = embedding_dim \n",
    "\n",
    "        # Embedding layer: Maps word indices to dense vectors \n",
    "        self.embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
    "\n",
    "        # Fully connected layer: Maps word indices to dense vectors \n",
    "        self.linear1 = nn.Linear(context_size * embedding_dim, 128)\n",
    "\n",
    "        # Output layer: Maps the hidden layer output to vocabulary size (probability distribution over words)\n",
    "        self.linear2 = nn.Linear(128, vocab_size)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        \"\"\"\n",
    "        Forward pass of the model.\n",
    "\n",
    "        Parameters:\n",
    "        inputs (Tensor): A tensor of shape (batch_size, context_size) containing word indices.\n",
    "\n",
    "        Returns:\n",
    "        Tensor: A tensor of shape (batch_size, vocab_size) representing predicted probabilities for the next word.\n",
    "        \"\"\"\n",
    "\n",
    "        # Convert input word indices into dense vectors using the embedding layer \n",
    "        embeds = self.embeddings(inputs) # Shape: (batch_size, context_size, embedding_dim)\n",
    "\n",
    "        # Reshape the embeddings into a single vector per input sample \n",
    "        embeds = torch.reshape(embeds, (-1, self.context_size * self.embedding_dim))\n",
    "        # New shape: (batch_size, context_size * embedding_dim)\n",
    "\n",
    "        # Apply first fully connected layer with ReLU activation \n",
    "        out = F.relu(self.linear1(embeds)) # Shape: (batch_size, 128)\n",
    "\n",
    "        # Apply second fully connected layer to generate vocabulary-size logits \n",
    "        out = self.linear2(out)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "348cdb50",
   "metadata": {},
   "source": [
    "Create a model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "f3ca3352",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = NGramLanguageModeler(len(vocab), EMBEDDING_DIM, CONTEXT_SIZE).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "569e9ab6",
   "metadata": {},
   "source": [
    "Retrieve samples from the data loader object and input them into the neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "6dbdc2e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[70, 58, 21],\n",
      "        [74, 70, 58],\n",
      "        [25, 74, 70],\n",
      "        [69, 25, 74],\n",
      "        [ 2, 69, 25],\n",
      "        [20,  2, 69],\n",
      "        [31, 20,  2]]) tensor([74, 25, 69,  2, 20, 31, 72])\n"
     ]
    }
   ],
   "source": [
    "context, target = next(iter(dataloader))\n",
    "print(context, target)\n",
    "out = model(context)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "964b1af7",
   "metadata": {},
   "source": [
    "Retrieve samples from the data loader objext and input them into the neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "4a24a5a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[70, 58, 21],\n",
      "        [74, 70, 58],\n",
      "        [25, 74, 70],\n",
      "        [69, 25, 74],\n",
      "        [ 2, 69, 25],\n",
      "        [20,  2, 69],\n",
      "        [31, 20,  2]]) tensor([74, 25, 69,  2, 20, 31, 72])\n"
     ]
    }
   ],
   "source": [
    "context, target = next(iter(dataloader))\n",
    "print(context, target)\n",
    "out = model(context)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d944bf4",
   "metadata": {},
   "source": [
    "While the model remains untrained, analyzing the output can provide us with a clearer understanding. In the output, the first dimension corresoponds to the batch size, while the second dimension represents the probability associated with each class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "aa72ba5c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([7, 79])"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d165075a",
   "metadata": {},
   "source": [
    "Find the index with the highest probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "b4bcc44c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([28, 57, 57, 56, 11, 16, 48])"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicted_index = torch.argmax(out, 1)\n",
    "predicted_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "98297be2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i', 'any', 'any', 'your', 'desert', 'let', 'on']"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[index_to_token[i.item()] for i in predicted_index]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb6a964e",
   "metadata": {},
   "source": [
    "We will create a function that accomplished the same task for the tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25c28386",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_song(model, my_song, number_of_words = 100):\n",
    "    \"\"\"\n",
    "    Generates text using a trained n-gram language model.\n",
    "\n",
    "    Given an initial text (`my_song`), the function generates additional words by \n",
    "    predicting the next word iteratively based on the trained model.\n",
    "\n",
    "    Parameters:\n",
    "    model (nn.Module): The trained n-gram language model.\n",
    "    my_song (str): The initial seed text to start generating words.\n",
    "    number_of_words (int): The number of words to generate (default: 100).\n",
    "\n",
    "    Returns:\n",
    "    str: The generated song lyrics as a string.\n",
    "    \"\"\"\n",
    "\n",
    "    # Get the mapping from index to word for decoding predictions\n",
    "    index_to_token = vocab.get_itos()\n",
    "\n",
    "    # Loop to generate from index to word for decoding predictions \n",
    "    for i in range(number_of_words):\n",
    "        with torch.no_grad(): # Disable a gradient computation for inference \n",
    "            # Prepare the input context by extracting the last CONTEXT_SIZE words from tokens \n",
    "            context = torch\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.25"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
